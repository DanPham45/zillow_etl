{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# DIMENSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from s3fs) (0.8.5)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from s3fs) (1.2.1)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: botocore<1.19.53,>=1.19.52 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (1.19.52)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs) (1.12.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.9.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.6/site-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install s3fs\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "from time import time\n",
    "import configparser\n",
    "\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "KEY=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET= config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "DWH_DB= config.get(\"DWH\",\"DB_NAME\")\n",
    "DWH_DB_USER= config.get(\"DWH\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD= config.get(\"DWH\",\"DB_PASSWORD\")\n",
    "DWH_PORT = config.get(\"DWH\",\"DB_PORT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DWH_ENDPOINT=\"dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com\"\n",
    "DWH_ROLE_ARN=\"arn:aws:iam::451089474087:role/dwhRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@capstone'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS zillow;\n",
    "\n",
    "SET\n",
    "search_path TO zillow;\n",
    "\n",
    "DROP TABLE IF EXISTS heating_or_system_type;\n",
    "\n",
    "DROP TABLE IF EXISTS property_land_use_type;\n",
    "\n",
    "DROP TABLE IF EXISTS story_type;\n",
    "\n",
    "DROP TABLE IF EXISTS air_conditioning_type;\n",
    "\n",
    "DROP TABLE IF EXISTS architectural_style_type;\n",
    "\n",
    "DROP TABLE IF EXISTS type_construction_type;\n",
    "\n",
    "DROP TABLE IF EXISTS building_class_type;\n",
    "\n",
    "\n",
    "CREATE TABLE heating_or_system_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\theating_or_system \tvarchar(30)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "CREATE TABLE property_land_use_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\tproperty_land_use \tvarchar(100)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "CREATE TABLE story_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\tstory \tvarchar(100)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "CREATE TABLE air_conditioning_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\tair_conditioning \tvarchar(30)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "CREATE TABLE architectural_style_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\tarchitectural_style \tvarchar(30)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "CREATE TABLE type_construction_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\ttype_construction \tvarchar(30)\tnot null\n",
    "\t)\n",
    "diststyle all;\n",
    "\t\n",
    "\n",
    "CREATE TABLE building_class_type (\n",
    "\tid\tinteger\tnot null,\n",
    "\tbuilding_class \tvarchar(300)\tnot null\n",
    "\t)\n",
    "diststyle all;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://capstone-rawdata/zillow_data_dimension_dictionary.xlsx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = \"s3://capstone-rawdata/\"\n",
    "# read dimension data\n",
    "dimension_data = os.path.join(input_data, \"zillow_data_dimension_dictionary.xlsx\")\n",
    "dimension_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HeatingOrSystemType', 'PropertyLandUseType', 'StoryType', 'AirConditioningType', 'ArchitecturalStyleType', 'TypeConstructionType', 'BuildingClassType']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "xl = pd.ExcelFile(dimension_data)\n",
    "print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(dimension_data)\n",
    "#print(xl.sheet_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeatingOrSystemType\n",
      "heating_or_system_type\n",
      "PropertyLandUseType\n",
      "property_land_use_type\n",
      "StoryType\n",
      "story_type\n",
      "AirConditioningType\n",
      "air_conditioning_type\n",
      "ArchitecturalStyleType\n",
      "architectural_style_type\n",
      "TypeConstructionType\n",
      "type_construction_type\n",
      "BuildingClassType\n",
      "building_class_type\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "conn = create_engine(conn_string)\n",
    "tables = ['heating_or_system_type','property_land_use_type','story_type','air_conditioning_type','architectural_style_type','type_construction_type','building_class_type']\n",
    "for i, sheet in enumerate(xl.sheet_names):\n",
    "    d[f'{sheet}']= pd.read_excel(xl,sheet_name=sheet)\n",
    "    df = d[f'{sheet}']\n",
    "    print(sheet)\n",
    "    print(tables[i])\n",
    "    df.to_sql(tables[i], conn, index=False, if_exists='append', schema='zillow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# TRANSACTION TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "jars = [\n",
    "    \"/home/workspace/RedshiftJDBC42-no-awssdk-1.2.51.1078.jar\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \":\".join(jars)) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\\\n",
    "    .config(\"com.amazonaws.services.s3.enableV4\", True)\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\")\\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dcom.amazonaws.services.s3.enableV4=true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=1, air_conditioning='Central')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:redshift://dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\").option(\"driver\",\"com.amazon.redshift.jdbc42.Driver\").option(\"dbtable\",\"zillow.air_conditioning_type\").option(\"user\", DWH_DB_USER).option(\"password\", DWH_DB_PASSWORD).load()\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read from S3, write back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket(name='capstone-rawdata')\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "## Bucket to use\n",
    "bucket = s3.Bucket('capstone-rawdata')\n",
    "print(bucket)\n",
    "json = []\n",
    "print(type(json))\n",
    "csv = []\n",
    "## List objects within a given prefix\n",
    "for obj in bucket.objects.filter(Delimiter='/'):\n",
    "    if 'json' in obj.key:\n",
    "        json.append(obj.key)\n",
    "    if 'sample1.csv' in obj.key:\n",
    "        csv.append(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transactions_2016.json', 'transactions_2017.json']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2sample1.csv', 'sample1.csv']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://capstone-rawdata/transactions_*.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transactions_data = os.path.join(input_data, '_*.json')\n",
    "print(transactions_data)\n",
    "# read transaction data:\n",
    "tdf = spark.read.option(\"multiline\",\"true\").json(transactions_data)\n",
    "# clean data\n",
    "tdf = tdf.withColumn(\"transaction_date\", expr(\"TO_DATE(CAST(UNIX_TIMESTAMP(transactiondate, 'yyyy-MM-dd') AS TIMESTAMP))\"))\n",
    "\n",
    "tdf = tdf.select(col(\"parcelid\").alias(\"parcel_id\"),col(\"logerror\").alias(\"log_error\"), col(\"transaction_date\"))\n",
    "tdf.head()\n",
    "# save to S3\n",
    "tdf.write.format('parquet').bucketBy(10, \"parcel_id\")\\\n",
    ".sortBy(\"parcel_id\").option(\"path\",os.path.join(output_data, 'transactions'))\\\n",
    ".saveAsTable('zillowwwtransactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://capstone-rawdata/transactions_2016.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = \"s3a://capstone-rawdata/\"\n",
    "transactions_data = os.path.join(input_data, \"transactions_2016.json\")\n",
    "transactions_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(parcel_id=11016594, log_error=0.0276, transaction_date=datetime.date(2016, 1, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(parcel_id=11016594, log_error=0.0276, transaction_date=datetime.date(2016, 1, 1)),\n",
       " Row(parcel_id=14366692, log_error=-0.1684, transaction_date=datetime.date(2016, 1, 1)),\n",
       " Row(parcel_id=12098116, log_error=-0.004, transaction_date=datetime.date(2016, 1, 1)),\n",
       " Row(parcel_id=12643413, log_error=0.0218, transaction_date=datetime.date(2016, 1, 2)),\n",
       " Row(parcel_id=14432541, log_error=-0.005, transaction_date=datetime.date(2016, 1, 2))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"s3a://zillowanalytics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Copy from S3 to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS zillow;\n",
    "\n",
    "SET search_path TO zillow;\n",
    "DROP TABLE IF EXISTS property_transactions;\n",
    "\n",
    "CREATE TABLE property_transactions (\n",
    "\tparcel_id\tbigint\tnot null,\n",
    "\tlog_error \tfloat\tnot null,\n",
    "\ttransaction_date\tdate\tnot null\n",
    "\t)\n",
    "diststyle all;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = \"\"\"\n",
    "    copy property_transactions \n",
    "    from 's3://zillowanalytics/transactions/'\n",
    "    iam_role '{}'\n",
    "    format as parquet;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# PROPERTY DETAILS TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3a://capstone-rawdata/*sample1.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties_data = os.path.join(input_data, \"*sample1.csv\")\n",
    "properties_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read property data\n",
    "pdf = spark.read.option(\"header\",\"true\").csv(properties_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(parcelid=10754147, airconditioningtypeid=None, architecturalstyletypeid=None, bathroomcnt=0, bedroomcnt=0, buildingclasstypeid=None, calculatedfinishedsquarefeet=None, heatingorsystemtypeid=None, propertylandusetypeid=269, regionidcity=37688, regionidcounty=3101, regionidzip=96337, roomcnt=0, storytypeid=None, typeconstructiontypeid=None, yearbuilt=None, taxvaluedollarcnt=9)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "drop_columns = ['basementsqft','buildingqualitytypeid','calculatedbathnbr','decktypeid','finishedfloor1squarefeet',\n",
    "                'finishedsquarefeet12','finishedsquarefeet13','finishedsquarefeet15','finishedsquarefeet50','finishedsquarefeet6',\n",
    "                'fips','fireplacecnt','fullbathcnt','garagecarcnt','garagetotalsqft','hashottuborspa','latitude','longitude',\n",
    "                'lotsizesquarefeet','poolcnt','poolsizesum','pooltypeid10','pooltypeid2','pooltypeid7','propertycountylandusecode',\n",
    "                'propertyzoningdesc','rawcensustractandblock','regionidneighborhood','threequarterbathnbr', 'unitcnt','yardbuildingsqft17',\n",
    "                'yardbuildingsqft26','numberofstories','fireplaceflag','structuretaxvaluedollarcnt','assessmentyear','landtaxvaluedollarcnt',\n",
    "                'taxamount','taxdelinquencyflag','taxdelinquencyyear','censustractandblock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "pdf = reduce(DataFrame.drop, drop_columns, pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(parcelid='10754147', airconditioningtypeid=None, architecturalstyletypeid=None, bathroomcnt='0', bedroomcnt='0', buildingclasstypeid=None, calculatedfinishedsquarefeet=None, heatingorsystemtypeid=None, propertylandusetypeid='269', regionidcity='37688', regionidcounty='3101', regionidzip='96337', roomcnt='0', storytypeid=None, typeconstructiontypeid=None, yearbuilt=None, taxvaluedollarcnt='9')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type  count\n",
       "0  int  17   "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', -1) # to prevent truncating of columns in jupyter\n",
    "\n",
    "def count_column_types(spark_df):\n",
    "    \"\"\"Count number of columns per type\"\"\"\n",
    "    return pd.DataFrame(spark_df.dtypes).groupby(1, as_index=False)[0].agg({'count':'count'}).rename(columns={1:\"type\"})\n",
    "\n",
    "count_column_types(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "int_columns = ['parcelid','airconditioningtypeid','architecturalstyletypeid','bathroomcnt','bedroomcnt',\n",
    "                 'buildingclasstypeid','calculatedfinishedsquarefeet','heatingorsystemtypeid','propertylandusetypeid',\n",
    "                'regionidcity','regionidcounty','regionidzip','roomcnt','storytypeid','typeconstructiontypeid','yearbuilt','taxvaluedollarcnt']\n",
    "\n",
    "#float_columns = ['calculatedfinishedsquarefeet','tax_value_dollar_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "pdf = pdf.select(*(col(c).cast(\"int\").alias(c) for c in pdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 17)\n"
     ]
    }
   ],
   "source": [
    "#check shape\n",
    "print((pdf.count(), len(pdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#drop columns with null parcelid\n",
    "pdf = pdf.na.drop(subset=[\"parcelid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(pdf)['count'][0] == 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_column_types(pdf)['type'][0] == 'int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_column_types(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "null_count = pdf.filter(pdf.parcelid.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_null_value(df):\n",
    "    \"\"\"Count number of null value\"\"\"\n",
    "    return df.filter(df.parcelid.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_null_value(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-ddd78d9d5f8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# save clean result back to S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucketBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parcelid\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parcelid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'propdetails'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'output_data' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# data quality check: \n",
    "if count_column_types(pdf)['count'][0] != 17:\n",
    "    sys.exit(\"Did not have 17 columns as defined\")\n",
    "elif count_column_types(pdf)['type'][0] != 'int':\n",
    "    sys.exit(\"Did not have int type\")\n",
    "elif count_null_value(pdf) != 0:\n",
    "    sys.exit(\"Parcelid contains null value\")\n",
    "else:\n",
    "    # save clean result back to S3\n",
    "    pdf.write.format('parquet').bucketBy(10, \"parcelid\")\\\n",
    "    .sortBy(\"parcelid\").option(\"path\",os.path\\\n",
    "    .join(output_data, 'propdetails')).saveAsTable('properties')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pdf.write.format('parquet').bucketBy(10, \"parcelid\").sortBy(\"parcelid\").option(\"path\",os.path.join(output_data, 'propdetails')).saveAsTable('ppproperties')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS zillow;\n",
    "\n",
    "SET search_path TO zillow;\n",
    "DROP TABLE IF EXISTS property_details;\n",
    "\n",
    "CREATE TABLE property_details (\n",
    "\tparcel_id\tbigint not null,\n",
    "\tair_conditioning_type_id\tint,\n",
    "\tarchitectural_style_type_id \tint,\n",
    "\tbathroom_count\tint,\n",
    "\tbedroom_count\tint,\n",
    "\tbuilding_class_type_id\tint,\n",
    "\tcalculated_finished_square_feet int,\n",
    "\theating_or_system_type_id int,\n",
    "\tproperty_land_use_type_id int,\n",
    "\tregion_id_city int,\n",
    "\tregion_id_county int,\n",
    "\tregion_id_zip int,\n",
    "\troom_count int,\n",
    "\tstory_type_id int, \n",
    "\ttype_construction_type_id int,\n",
    "\tyear_built int,\n",
    "\ttax_value_dollar_count bigint\n",
    "\t)\n",
    "diststyle even;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = \"\"\"\n",
    "    copy property_details \n",
    "    from 's3://zillowanalytics/propdetails/'\n",
    "    iam_role '{}'\n",
    "    format as parquet;\n",
    "\"\"\".format(DWH_ROLE_ARN)\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ANALYTICS TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Read data from Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(parcel_id=10711738, log_error=0.0276, transaction_date=datetime.date(2016, 8, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:redshift://dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\").option(\"driver\",\"com.amazon.redshift.jdbc42.Driver\").option(\"dbtable\",\"property_transactions\").option(\"user\", DWH_DB_USER).option(\"password\", DWH_DB_PASSWORD).load()\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(parcel_id=10879947, air_conditioning_type_id=None, architectural_style_type_id=None, bathroom_count=0, bedroom_count=0, building_class_type_id=4, calculated_finished_square_feet=1776, heating_or_system_type_id=None, property_land_use_type_id=31, region_id_city=12447, region_id_county=3101, region_id_zip=96450, room_count=0, story_type_id=None, type_construction_type_id=None, year_built=1947, tax_value_dollar_count=433491)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:redshift://dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\").option(\"driver\",\"com.amazon.redshift.jdbc42.Driver\").option(\"dbtable\",\"property_details\").option(\"user\", DWH_DB_USER).option(\"password\", DWH_DB_PASSWORD).load()\n",
    "details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "details = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:redshift://{}:{}/{}\".format(DWH_ENDPOINT,config.get(\"DWH\",\"DB_PORT\"),config.get(\"DWH\",\"DB_NAME\"))).option(\"driver\",\"com.amazon.redshift.jdbc42.Driver\").option(\"dbtable\",\"property_details\").option(\"user\", config.get(\"DWH\",\"DB_USER\")).option(\"password\", config.get(\"DWH\",\"DB_PASSWORD\")).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "trans.createOrReplaceTempView(\"transactions\")\n",
    "details.createOrReplaceTempView(\"details\")\n",
    "\n",
    "analytics_table = spark.sql(\"\"\"\n",
    "        SELECT d.region_id_county,\n",
    "        extract(year from t.transaction_date) as year,\n",
    "        extract(month from t.transaction_date) as month,\n",
    "        count(distinct t.parcel_id) as transaction_count,\n",
    "        sum(power(2.71828,(ln(cast(d.tax_value_dollar_count as float))+t.log_error))) as transaction_value\n",
    "        FROM transactions t \n",
    "        LEFT JOIN details d \n",
    "        ON t.parcel_id = d.parcel_id\n",
    "        group by d.region_id_county,\n",
    "        extract(year from t.transaction_date),\n",
    "        extract(month from t.transaction_date)\n",
    "    \"\"\")\n",
    "\n",
    "# check if a transaction not in details table --> exclude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(region_id_county=None, year=2017, month=3, transaction_count=9327, transaction_value=None),\n",
       " Row(region_id_county=None, year=2017, month=8, transaction_count=9934, transaction_value=None),\n",
       " Row(region_id_county=None, year=2016, month=7, transaction_count=9947, transaction_value=None),\n",
       " Row(region_id_county=None, year=2016, month=11, transaction_count=1826, transaction_value=None),\n",
       " Row(region_id_county=None, year=2016, month=5, transaction_count=9961, transaction_value=None)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>count</th>\n",
       "      <th>names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bigint</td>\n",
       "      <td>1</td>\n",
       "      <td>transaction_count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>double</td>\n",
       "      <td>1</td>\n",
       "      <td>transaction_value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>int</td>\n",
       "      <td>3</td>\n",
       "      <td>year | month | region_id_county</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type  count                            names\n",
       "0  bigint  1      transaction_count              \n",
       "1  double  1      transaction_value              \n",
       "2  int     3      year | month | region_id_county"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_column_types(spark_df):\n",
    "    \"\"\"Count number of columns per type\"\"\"\n",
    "    return pd.DataFrame(spark_df.dtypes).groupby(1, as_index=False)[0].agg({'count':'count', 'names': lambda x: \" | \".join(set(x))}).rename(columns={1:\"type\"})\n",
    "\n",
    "count_column_types(analytics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-24b8eb8dcbf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalytics_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"region_id_county\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'transactions_by_month'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "analytics_table.write.partitionBy(\"region_id_county\").mode('overwrite').parquet(os.path.join(output_data, 'transactions_by_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@dwhcluster.cug0uxavqpyn.us-west-2.redshift.amazonaws.com:5439/capstone\n",
      "Done.\n",
      "Done.\n",
      "Done.\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS zillow;\n",
    "\n",
    "SET search_path TO zillow;\n",
    "DROP TABLE IF EXISTS transactions_by_month;\n",
    "\n",
    "CREATE TABLE transactions_by_month (\n",
    "\tregion_id_county\tint,\n",
    "\tyear\tint\tnot null,\n",
    "\tmonth\tint\tnot null,\n",
    "\ttransaction_count\tint\tnot null,\n",
    "\ttransaction_value\tfloat\n",
    "\t)\n",
    "diststyle even;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
